{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:06:06.939834 10444 file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "I0625 15:06:09.682842 10444 file_utils.py:55] TensorFlow version 2.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fast_bert.data_cls import BertDataBunch\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.data_lm import BertLMDataBunch\n",
    "from fast_bert.learner_lm import BertLMLearner\n",
    "from fast_bert.metrics import fbeta, roc_auc\n",
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "device_cuda = torch.device(\"cuda\")\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpu = True\n",
    "else:\n",
    "    multi_gpu = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('./data/')\n",
    "LOG_PATH = Path('./logs/')\n",
    "MODEL_PATH = Path('./model/')\n",
    "LABEL_PATH = Path('./labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msg = tf.constant('Hello, TensorFlow!')\n",
    "tf.print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 11.8 GB\n",
      "Allocated: 11.8 GB\n",
      "Allocated: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.cuda.FloatTensor(10000)\n",
    "print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/10243,1), \"GB\")\n",
    "\n",
    "b = torch.cuda.FloatTensor(20000)\n",
    "print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/10243,1), \"GB\")\n",
    "torch.rand(20000,20000).cuda()\n",
    "print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/1024**3,1), \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = df.sample(frac=0.2, replace=False, random_state=42)\n",
    "train_set = df.drop(index = val_set.index)\n",
    "print('Nombre de commentaires dans le val_set:',len(val_set))\n",
    "print('Nombre de commentaires dans le train_set:', len(train_set))\n",
    "val_set.to_csv('./data/val_set.csv')\n",
    "train_set.to_csv('.data/train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.columns[2:].to_list()\n",
    "with open('./labels/labels.txt', 'w') as f:\n",
    "    for i in labels:\n",
    "        f.write(i + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_texts = pd.read_csv('./data/raw_data.csv')\n",
    "df_texts = pd.read_csv('./data/autres/df_mg36_2020.csv', sep='|', encoding ='utf-8')\n",
    "#all_texts = df_texts[''].to_list()\n",
    "#print('Nombre de commentaires:', len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts['fst_txt']=df_texts['fst_txt'].str[13:]\n",
    "del df_texts['texte_ss_stp']\n",
    "df_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts.to_csv('./data/df_mg_37.csv', sep='|', encoding ='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvt_val_set = df_texts.sample(frac=0.2, replace=False, random_state=42)\n",
    "bvt_train_set = df_texts.drop(index = bvt_val_set.index)\n",
    "print('Nombre de commentaires dans le val_set:',len(bvt_val_set))\n",
    "print('Nombre de commentaires dans le train_set:', len(bvt_train_set))\n",
    "bvt_val_set.to_csv('./data/bvt_val_set.csv',sep='|', encoding ='utf-8',index=False)\n",
    "bvt_train_set.to_csv('./data/bvt_train_set.csv',sep='|', encoding ='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a partir d'ICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lecture direct des données preparées\n",
    "df_texts=pd.read_csv('./data/df_mg_37.csv', sep='|', encoding ='utf-8',index_col=None)\n",
    "df_texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts.fst_txt.str.replace(\"[<>%\\$]\", '')\n",
    "df_texts.fst_txt.str.replace(\"[u'\\u2260']\", '<>')\n",
    "df_texts.fst_txt.str.replace(u'œ', u'oe')\n",
    "df_texts.fst_txt.str.replace(u'μ', u'mu')\n",
    "df_texts.fst_txt.str.replace(u'≠', u'<>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts=df_texts['fst_txt'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(sentence):\n",
    "    return ' '.join([i for i in sentence.split() if i.isalpha()])\n",
    "all_texts=[removePunctuation(x) for x in all_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correction des codes non pris en compte dans unidecode\n",
    "#retire le Œ mu\"\n",
    "all_texts = [sub.replace(u'œ', u'oe') for sub in all_texts] \n",
    "all_texts = [sub.replace(u'μ', u'mu') for sub in all_texts]\n",
    "all_texts = [sub.replace(u'≠', u'<>') for sub in all_texts]\n",
    "all_texts = [sub.replace(u'\\u2260', u'<>') for sub in all_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrige une bonne partie des codes à problemes mais pas tous\n",
    "all_texts=[unidecode(x) for x in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "#all_texts=[ftfy.fix_text(x) for x in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_texts=[ftfy.fix_encoding(x) for x in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "#all_texts=[unicodedata.normalize('NFD',x).encode('utf-8', errors='ignore') for x in all_texts]\n",
    "#[unicodedata.is_normalized('NFD', x) for x in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "s = \"Héllô œ μ Càèùverâêt Jîôûç ïîäüë ≠\"\n",
    "s = unidecode(s)\n",
    "print(s) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stre=[\"ma chaine de œ pose probleme\",\" ici c ≠ est μ le soucis\"]\n",
    "print(stre)\n",
    "stre = [sub.replace(u'œ', u'oe') for sub in stre]  \n",
    "stre = [sub.replace(u'μ', u'mu') for sub in stre]\n",
    "stre = [sub.replace(u'\\u2260', u'<>') for sub in stre]\n",
    "\n",
    "print (stre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_texts[0]) # en utf-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_texts[2])  #cp1252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./alltexts.pkl', 'wb') as f:\n",
    "    pickle.dump(all_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./alltexts.pkl', 'rb') as f:\n",
    "    all_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de LMDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# la première fois\n",
    "databunch_lm = BertLMDataBunch.from_raw_corpus(\n",
    "                    data_dir=DATA_PATH,\n",
    "                    text_list=all_texts,\n",
    "                    tokenizer='camembert-base',\n",
    "                    batch_size_per_gpu=16,\n",
    "                    max_seq_length=512,\n",
    "                    multi_gpu=False,\n",
    "                    model_type='camembert-base',\n",
    "                    logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:06:22.948308 10444 tokenization_utils.py:1022] loading file https://s3.amazonaws.com/models.huggingface.co/bert/camembert-base-sentencepiece.bpe.model from cache at C:\\Users\\odissaux/.cache\\torch\\transformers\\3715e3a4a2de48834619b2a6f48979e13ddff5cabfb1f3409db689f9ce3bb98f.28d30f926f545047fc59da64289371eef0fbdc0764ce9ec56f808a646fcfec59\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n",
      "I0625 15:06:23.044310 10444 data_lm.py:129] Creating features from dataset file data\\lm_train.txt\n"
     ]
    }
   ],
   "source": [
    "#les fois suivantes repartir d'ici , il utilise les fichiers lm_train et lm_test générés depuis la cellule précédente\n",
    "databunch_lm=BertLMDataBunch(data_dir=DATA_PATH, \n",
    "                    tokenizer='camembert-base',\n",
    "                    batch_size_per_gpu=32,\n",
    "                    max_seq_length= 512,\n",
    "                    multi_gpu=False,\n",
    "                    model_type='camembert-base',\n",
    "                    logger=None, \n",
    "                    clear_cache=False,\n",
    "                    no_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de LMLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_learner = BertLMLearner.from_pretrained_model(\n",
    "                            dataBunch=databunch_lm,\n",
    "                            pretrained_path='camembert-base',\n",
    "                            output_dir=MODEL_PATH,\n",
    "                            metrics=[],\n",
    "                            device=device_cuda,\n",
    "                            logger=logger,\n",
    "                            multi_gpu=False,\n",
    "                            logging_steps=50,\n",
    "                            fp16_opt_level=\"O2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_learner.fit(epochs=30,\n",
    "            lr=1e-4,\n",
    "            validate=True,\n",
    "            schedule_type=\"warmup_cosine\",\n",
    "            optimizer_type=\"adamw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de databunch pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH,\n",
    "                          tokenizer='camembert-base',\n",
    "                          train_file='bvt_train_set.csv',\n",
    "                          val_file='bvt_val_set.csv',\n",
    "                          label_file='labels_poles.txt',\n",
    "                          text_col='fst_txt',\n",
    "                          label_col=['pole'],\n",
    "                          batch_size_per_gpu=16,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=True,\n",
    "                          model_type='camembert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [{'name': 'fbeta', 'function': fbeta}, {'name': 'roc_auc', 'function': roc_auc}]\n",
    "OUTPUT_DIR = Path('./finetuned_model')\n",
    "WGTS_PATH = Path('model/model_out/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_learner = BertLearner.from_pretrained_model(\n",
    "                        databunch,\n",
    "                        pretrained_path='model/model_out',\n",
    "                        metrics=metrics,\n",
    "                        device=device_cuda,\n",
    "                        logger=logger,\n",
    "                        output_dir=OUTPUT_DIR,\n",
    "                        finetuned_wgts_path=WGTS_PATH,\n",
    "                        warmup_steps=300,\n",
    "                        multi_gpu=False,\n",
    "                        multi_label=True,\n",
    "                        is_fp16=True,\n",
    "                        logging_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_learner.fit(epochs=30,\n",
    "            lr=9e-5,\n",
    "            validate=True,\n",
    "            schedule_type=\"warmup_cosine\",\n",
    "            optimizer_type=\"adamw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_learner.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = BertClassificationPredictor(\n",
    "                model_path='finetuned_model/model_out',\n",
    "                label_path='labels/',\n",
    "                multi_label=True,\n",
    "                model_type='camembert-base',\n",
    "                do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(\"Texte à classer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.get_objects()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
